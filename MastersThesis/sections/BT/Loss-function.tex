\subsection{Loss functions*}
\label{section:BT:Loss}
\citeauthor{Russel2012}  defines loss functions as such:

\begin{quotation}
  A \textit{loss function} $L(x, y, \hat{y})$ is defined as the amount of utility lost by predicting
  $h(x)=\hat{y}$ when the correct answer is $f(x) = y$ and $h$ is the heuristic function.
  This is the most general formulation of the loss function. Often a simplified version is used,
  $L(y, \hat{y})$, that is independent of x \cite[p. 710-711]{Russel2012}. 
\end{quotation}

This means that the loss function is the function that calculates the error between the
models prediction, and the actual target value.
This chapter will briefly explain standard loss functions.


\subsubsection{MSE}
% Common loss functions
The most commonly used loss function for regression problems is the
\textbf{Mean Squared Error (MSE)} function in \Cref{eq:mean-squared-error}.
It is the mathematically preferred function if the target distribution is Gaussian.
It punishes large errors much more harshly than smaller errors due to its squaring of the error.
Here $e = y - \hat{y}$, where $y$ is the actual value and $\hat{y}$ is the predicted value.

\begin{equation}
  \label{eq:mean-squared-error}
  MSE = \frac{1}{n} \sum_{t=1}^n e_t^2
\end{equation}

\subsubsection{MAE}
If the target distribution consists of outliers, then the
\textbf{Mean Absolute Error (MAE)} in \Cref{eq:mean-absolute-error} is more appropriate
as it does not punish the outliers too much.

\begin{equation}
  \label{eq:mean-absolute-error}
  MSE = \frac{1}{n} \sum_{t=1}^n |e_t|
\end{equation}

\subsubsection{MAPE}
The Mean Absolute Percentage Error \Cref{eq:Mape} (MAPE) is a popular metric for evaluating forecasting performance.
$y$ is the actual target value of the target we are trying to forecast, and $\hat{y}$ is the predicted value.
$t$ is the time index.

The advantages of MAPE is that it is expressed as a percentage, which means
it is scale-independent and can be used for forecasting on different scales.
A percentage is also easily explainable.
\begin{equation}
  \label{eq:Mape}
  MAPE = \frac{1}{n} \sum_{t=1}^n \frac{|\hat{y_t} - y_t|}{(y_t)}
\end{equation}

A significant shortcoming of MAPE is that it is undefined when the actual value $y$ is 0.
It will also produce extreme values if the value is close to 0.

MAPE is also asymmetric and puts a higher penalty on errors where the predicted
$\hat{y}$ is higher than the actual value $y$.
This is because as long as we are dealing with positive numbers,
the highest bound for a low forecast is 100\%. But there is no upper limit
for forecasts that are too high. As a result, the error function will
favor models that under-predict rather than over-predict a forecast.

\subsubsection{SMAPE}
Symmetric Mean Absolute Percentage Error (SMAPE) shown in \Cref{eq:sMape}
is an error function made to overcome some of the shortcomings of the MAPE function.
By incorporating $\hat{y}$ to the denominator, SMAPE is symmetrical,
with a lower bound of 0\% and an upper bound of 200\%.

SMAPE is one of the most used performance measures and is used in many forecasting competitions.

SMAPE is still vulnerable to denominator values close to zero.
\cite{Hewamalage2021} solves the zero problem by changing the denominator
of the SMAPE to $max(|y| + |\hat{y}| + \epsilon, 0.5 + \epsilon)$, where $\epsilon$
is set to 0.1.
This version of SMAPE avoids division by zero by switching to an alternate positive
constant for the denominator when the forecasting values are too small.

% Lack of interpretability and high skewnedd.

\begin{equation}
  \label{eq:sMape}
  SMAPE = \frac{1}{n} \sum_{t=1}^n \frac{|\hat{y_t} - y_t|}{(|y_t| + |\hat{y_t}|) / 2}
\end{equation}

\begin{equation}
  \label{eq:sMape-zero-division-alt}
  SMAPE_{ALT} = \frac{1}{n} \sum_{t=1}^n \frac{|\hat{y_t} - y_t|}{max(|y| + |\hat{y}| + \epsilon, 0.5 + \epsilon)}
\end{equation}



\subsubsection{MASE}
The Mean Absolute Scaled Error \textbf{MASE}
proposed by \cite{Hyndman2006}, is a scale-free
error metric that compares predictions with
the output of a Naive Forecast.


\begin{displaymath}
  MASE = \frac{MAE}{MAE_{in-sample, naive}}
\end{displaymath}
where $MAE$ is the mean absolute error
produced by the actual forecast and
$MAE_{in-sample, naive} $
is the mean absolute error produced by
a naive forecast, calculated on the in-sample data.
The naive forecast is predicting that the next
time step is equal to the actual value of the current time step.

%\begin{displaymath}
%  MAE_{naive} = \frac{1}{N-1} \sum_{t=2}^N |y_t - y_{t-1}|
%\end{displaymath}

\begin{displaymath}
  \label{eq:MASE}
  MASE = \frac{\frac{1}{J} \sum_{j=1}^n |\hat{y_j} - y_j|}{\frac{1}{T-1} \sum_{t=2}^n |y_t - y_{t-1}|}
\end{displaymath}
where

$J$ = Number of forecasts

$t$ = The training set

$T$ = Number of samples in the training set

\begin{equation}
  \label{eq:MASE-Seasonal}
  MASE = \frac{
    \overbrace{
      \frac{1}{J} \sum_{j=1}^n |\hat{y_j} - y_j|}^\text{MAE}}{
    \underbrace{
      \frac{1}{T-m} \sum_{t=m+1}^n |y_t - y_{t-m}|}_\text{$MAE_{in-sample, naive}$}
  }
\end{equation}
where:

$m$ = seasonal period

The main difference is the denominator is MAE
of the one-step seasonal naive forecast method on the training set.

MASE is a scale-independent measure, which makes it
a good choice for domains with multiple time series of
different scales.

MASE is normalized by the average in-sample one-step
seasonal forecast error. So a MASE value greater than
1 indicates that the model tested is worse compared
to the naive benchmark.
The closer the MASE error is to 0, the better the model.


