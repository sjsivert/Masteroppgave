
\subsection{Convolutional Autoencoder with LSTM}
\label{section:Discussion:Discussion:CNN-AE-LSTM}

% What was the aim of the experiments?
% --> Apply the CNN-AE and LSTM model to a new dataset with real world applicaitons
% --> Verrify the ability of ther hybrid model against the baseline LSTM
% --> Thus verrifying the claims done by "Zhao2019"


% Link this to RQ-5 (CNN-AE and LSTM against the baseline LSTM)
% The thesis is based on "Local univariate" models. We therefor first mention this
% All withing the margin of error
% We are not able to recreate the values by Zhao2019

% ....
% Dataset   -   LSTM   -  CNN
% Dataset 1 -  0.208   -  0.208 ->  0.0%
% Dataset 2 -  0.684   -  0.716 ->  -4.57%
% Dataset 3 -  0.477   -  0.483 ->  -1.25%

After doing experiments with the LSTM and the CNN-AE and LSTM model on the three datasets used in this thesis,
the results from the predictions are presented in \Cref{section:Results}.
Using the hybrid model, it is clear that we are not able to recreate the values presented in \cite{Zhao2019}
for each of the different datasets.

Experiments conducted on the different datasets result in predictions that can vary vastly,
and some that are quite similar between the models.

The local univariate LSTM and the local univariate convolutional autoencoder and LSTM are one of the model-configurations
that offer rather similar results across datasets.
While the hybrid model CNN-AE-LSTM performance is more or less equal, predictions made on datasets 2 and 3 vary a bit more
with the basic LSTM model performing, on average, 4.6\% and 1.25\% better on average.
However, in addition to these similarities in average performance, the T-test conducted on the experiments reveals the same outcome.
The t-test Resultsults shown in \Cref{table:ttest-p-values-main-experiments-sMAPE} infer that the predictions made by the two local univariate models
are from the same group, and there is no significant difference in their distribution.

The t-test and average metrics attained through the experimentation, therefore, infer that the hybrid model CNN-AE-LSTM
is not able to improve performance over the LSTM model on a local univariate model.


One reason behind this lack of difference in predictions might originate in the selected data.
The task of the convolutional autoencoder is to encode and reconstruct the input data of the model,
removing noise and other unneeded information in the process.
Lack of data could serve as an issue in this case.
As each of the time series used in the local univariate model only consists of around three years of data, or 1300 data points.
There is a possibility that there is not enough similar data for the autoencoder to recognize similarities in data
so that it can then remove the extra noise.
Additionally, the amount of noise contained in the data is not known.
If the datasets contain little noise, the autoencoder is more likely to remove relevant data, thus impairing the performance of the LSTM model.
Due to the implications that the lack of noise in the data potentially could infer from the predictions,
additional experiments are done on data with high noise and low noise.
We will come back to these experiments later in this discussion.

However, as is seen from the predictions made by the two models, it is clear that the convolutional autoencoder and lstm
does not have improved predictions compared to the LSTM model.
We are therefore not able to reproduce the results presented by \cite{Zhao2019}
which showed a significant improvement using the convolutional autoencoder and LSTM over the baseline LSTM model.


While the thesis \cite{Zhao2019} only focuses on applying local univariate models,
we wished to explore the expansion of the use of the hybrid model with other configurations.
These include the use of multivariate models and global models for each dataset.
Exploring the use of such variations to the Convolutional autoencoder and LSTM,
we hoped to uncover cases where the model performance is increased over the standard local univariate model.










\subsubsection{Local multivariate models}

After the use of global models, a configuration of multivariate models was attempted.
Unlike the global models, the amount of data is increased not through the addition of other time-series to the model,
but by decomposing the information within each time-series.
By decomposing information such as day of the week, or season, the amount of data supplied to the models is increased.


Using the hybrid convolutional autoencoder and LSTM model should result in both improvements and degradation in performance.

Initially, the performance of the model is likely to suffer due to the fact that
the model is required to encode and reconstruct additional input data using the same model structure.
The model might encounter problems recreating the data using the same model as for the univariate model
while attempting to recreate additional data per data point.
If the autoencoder then is limited by the number of data entries on top of this, the model performance could suffer.

However, the same reasoning might also help improve the hybrid model's performance in some situations.
While more data would need to be encoded and reconstructed, the autoencoder will retain more information
regarding the development of trends and spikes through different seasons.
% 
While a pattern might repeat over several seasons, there might be additional hidden information in the seasonal information.
If a pattern is known to never spike during summer but often spikes through the winter,
the autoencoder should be able to differentiate between the different seasons.
Thus, the task of the autoencoder would be to retain information about changes in trends also dependent on seasonal data.


% Second -> Use of local multivariate
% ....
% Dataset   -   LSTM   -  CNN
% Dataset 1 -  0.183   -  0.181  ->  1.1%
% Dataset 2 -  0.603   -  0.743  ->  -23.2%
% Dataset 3 -  0.359   -  0.354  ->  1.4%
However, while this reasoning is based on the autoencoder's ability to encode and differentiate between values based on season,
the amount of data would severely limit the model.
If there is not enough data available, the autoencoder would not be able to learn of such connections.
As discussed in \Cref{section:Data:DataExploration}, the data attained from ``Prisguiden.no'' only include data-points
for a little over three years.
As with the problems discussed with the local univariate model, the multivariate model would also likely be limited by the amount of data.

This is reflected in the results acquired through testing.
While dataset 1 and 3 implies slight improvements to predictions, there are no clear improvements.
Additionally, the t-test applied to the predictions supports this assumption.
The t-test implies that there are no significant differences between the predictions made with the local multivariate model.


However, the results are quite different for dataset 2.
Considering the sMAPE error metric there is about a 23\% performance decrease using the convolutional autoencoder and LSTM.
This is also supported by the t-test conducted. The results from the t-test imply that the difference between the hybrid model and the LSTM model
is significant within the 95\% window of certainty, supplied by a p-value of under 0.05.
Although this is only the case for dataset 2, the convolutional autoencoder and LSTM perform significantly worse than the LSTM model.
% Why is this?
The significant decrease in performance could be due to the lack of data as discussed earlier.
With the local univariate model, there is a 4\% performance decrease with the hybrid model.
Although this is not significant, this could point to the reason behind the poor performance.
While the local univariate model and the local multivariate models both suffer from a lack of data with each dataset.
Both models perform worse on dataset 2, while the performance on dataset 1 is somewhat better both using univariate and multivariate.
This could imply that the selected dataset is less suited for using the autoencoder.
This could then again imply that the convolutional autoencoder is heavily influenced by the nature of the data that is used.
The implications of the dataset's characteristics are explored further later when the use of noisy data is discussed.
However, if this is the case, the lack of data could explain the poor performance of dataset 2.
The multivariate model would have the same number of time-steps in each time-series but would be required to encode four times the data per time-step.
Therefore, the autoencoder is more likely to perform worse in reconstructing the input data,
thus contributing to a worse performance by the LSTM component.


Although the local multivariate model is the overall best performing of the convolutional autoencoder and LSTM models,
it is outperformed by the LSTM, indicating that the reason for the improvements by the model
is entirely contributed by the improved LSTM model.


\subsubsection{Global univariate models}
% ....
% Dataset   -   LSTM   -  CNN
% Dataset 1 -  0.203   -  0.210   ->  -3.4 %
% Dataset 2 -  0.662   -  0.675   ->  -1.96%
% Dataset 3 -  0.425   -  0.417   ->  1.9 %


\todo[inline]{---- This section reads a lot like a section from results, with little discussion...}
After having explored different local model configurations,
global models are also tested.
As is discussed in \cref{section:Discussion:Discussion:Global-v-local},
global univariate models generally outperform the local models, training on more available data for the neural network.
The same is mostly true for the convolutional autoencoder, which performs better with a global univariate configuration than with local univariate models.
This is the case for both dataset 2 and dataset 3, where the global univariate models, on average, performed way better.
The model performed marginally worse on dataset 1 on average. However, this is more or less neglectable due to the low difference and no clear significance either.

However, the performance between the convolutional autoencoder and LSTM, and the baseline LSTM model offers a different perspective.
The global univariate LSTM model generally performed better than the hybrid model.
Evaluating the difference with the t-test, it is also clear that the difference is significant.

% Dataset 1 is significantly worse (Can se this fom the individual dataset performances, where it is more or less always worse, and t-test says significanse)
% Dataset 2 is significantly different / worse (LSTM is generaly better, although there are more differences. Sometimes hybrid is better, but mostly, LSTM is best)
% Dataset 3 offers no significant difference. This could be contributed to the low sample size (only 8 time-series)
Although the average metrics performance between the hybrid model and the LSTM model on dataset 1 is only a little under 4\%,
there is still quite a difference.
By evaluating the metrics of each prediction in the dataset, as well as using the significance test results from the t-test,
it is clear that the global univariate LSTM model is significantly better than the global univariate convolutional autoencoder and LSTM for dataset 1.
For dataset 2 the difference is not as prominent as with dataset 1, but also here, there is a significant difference in predictions.
Only with dataset 3, where the hybrid model performs better than the LSTM model on average as a global univariate model,
the t-test signals that there are no significant differences between the two predictions.
However, it is unclear if this is due to the dataset or if this has to do with the low sample size compared to dataset 1 and dataset 2.
It is clear that with a global univariate configuration, the hybrid model is not able to outperform the LSTM model and often performs worse.
\todo[inline]{... End of section-----}


% Sindre sin forklaring pÃ¥ hvorfor Autoencoderen ikke er like god
It seems the benefits of a global model for RNNs are not transferable to other types
of NNs. LSTMs ability to learn across multiple time series can be attributed to the fundamentals
of how the LSTM works. \cite{Zhao2019} discusses previous work that concludes that
the LSTMs memory cell is mainly responsible for the performance of the LSTM.
When training the LSTM on multiple time series, this memory cell is local to each
time-series and resets before each new time series are fed through the network.
The LSTM weights, on the other hand, are global across all the series.
These local and global characteristics are unique to RNNs, and can explain
why the Convolutional Autoencoder does not see the same benefits when trained on multiple
time series.
In order for the autoencoder to work well on such time-series data,
there is a prerequisite that the data is connected and that information from one set can be applied to the others.
If this is not the case, the use of global models increasing the amount of independent data would only serve to decrease the performance of the autoencoder.







\subsubsection{Global Multivariate models}

% Third -> Use of global multivariate
% ....
% Dataset   -   LSTM   -  CNN
% Dataset 1 -  0.202   -  0.198  ->  1.98% -> Not significant
% Dataset 2 -  0.642   -  0.703  ->  -9.5% -> 0.0103 - Significant (both MASE and sMAPE)
% Dataset 3 -  0.399   -  0.566  ->  -41.85% -> 0.026 - Significant (both MASE and sMAPE)

Both multivariate models and global models have been tested with the hybrid convolutional autoencoder and LSTM.
The next step is to apply a model with both of these configurations.
We, therefore, apply a global multivariate convolutional autoencoder and LSTM, and measure the performance of the model
compared to a global multivariate LSTM model.

It is clear that the global multivariate model suffers the same problems that are prevalent in both the local multivariate model
and the global univariate model.
Like with the global univariate models, the autoencoder is not as well suited as predicted to encode multiple independent time series.
However, performance degradation is also influenced by using a multivariate model.
The autoencoder needs to encode and reconstruct more data per data-point, and with the same autoencoder structure
as well as a lack of data, the autoencoder will likely perform worse than with the use of a univariate model.

These assumptions are reflected in the results acquired from running experiments on the global multivariate models.
While the performance of dataset 1 is more or less equal between the baseline global multivariate LSTM model and the hybrid model,
the same is not the case for dataset 2 and dataset 3.
\todo[inline]{---- This section reads a lot like a section from results, with little discussion...}
On dataset 1 the hybrid model performs on average 2\% better than the LSTM model. However, this is not a significant difference in itself.
This is also supported by the results from the t-test, which implies there is no significance in the difference in predictions.
While there was a significant difference between the models using a global univariate model, the difference in performance was not high.
However, using the local multivariate model, the hybrid model performed somewhat better than the LSTM model for dataset 1.
It appears that the same applies to the global multivariate model, although it has not improved the performance of the hybrid model significantly
it seems that the use of a multivariate model has counteracted the adverse effects of the global model on dataset 1.

Unfortunately, the same does not apply to dataset 2 and dataset 3.
While the hybrid model, on average, perform about 9.5\% worse on dataset 2, it also performs about 41.9\% worse on dataset 3.
Applying the student t-test to the predictions also supports the notion of the global multivariate model decreasing performance for the
convolutional autoencoder and LSTM.
Both predictions for dataset 2 and dataset 3 are well within the confidence interval of the t-test,
thus implying a significant difference between the predictions for datasets 2 and 3.
\todo[inline]{... End of section ---}

Performance degradation for dataset 2 was expected due to the poor results both with the global univariate model and the local multivariate model.
Both of these configurations resulted in worse predictions on dataset 2.
Dataset 3 on the other hand, while expected, is not as easily explained.
While the performance was not particularly good on either of the previous models, it would appear that the combination of the multivariate model and global model
caused degradation in performance.
However, the vast difference in performance could also be attributed to the low sample size of the dataset.
While this is not the only reason, it might be a contributing factor to the large difference.

Either way, it is clear that the convolutional autoencoder is not well suited for use with global multivariate models, as it appears to
vastly decrease the performance of the predictions.








% High noise and low noise and low noise data
\subsubsection{CNN-AE-LSTM on high noise datasets}

As we discussed when talking about the performance of the convolutional autoencoder and LSTM on datasets 1, 2, and 3,
we made the assumption that the characteristics of the used dataset could have severe implications on the predictive accuracy of the hybrid model.

In order to test this, additional experiments were defined and run as described in \Cref*{section:Results:AdditionalExperimentalPlan}.
By applying these experiments using the hybrid model to 3 new datasets with varying degrees of data noise,
the assumption was that the degree of noise in the dataset would heavily influence the predictive abilities of the hybrid model.

Due to the design of the hybrid model, applying a convolutional autoencoder to the input data of the model,
the data would be altered in accordance with the autoencoder.
The assumption is that when applying the autoencoder to a dataset with a low amount of noise,
the autoencoder would be more likely to remove important information on which the LSTM part of the model is dependent on.
On the other hand, with a higher level of noise in the dataset,
the model is more likely to reduce the noisy values, contributing to input data that is easier for the LSTM model to interpret.

% What were the results from the predictions?
% Dataset    -  LSTM - CNN-AE         - T-test
% Low noise  - 0.421 - 0.641 - -52%   -> 0.010 < 0.05
% OK noise   - 0.502 - 0.501 - 0.19%  -> 0.7327 > 0.05
% High noise - 0.864 - 0.832 - 3.7%  -> 0.147 > 0.05

The results from the experiment substantiate the hypothesis above.
Using the first noise dataset (low noise), it is apparent that the CNN-AE-LSTM greatly decreases the performance of the predictions.
With a 52\% decrease in performance, it is clear that the model is not well suited to make predictions on such low noise data.
This notion is further supported by the t-test value signaling that the difference in predictions is significantly different.

However, applying the hybrid model to the ok-noise dataset is more successful.
Although it does not improve the accuracy and reduce the error metric of the predictions,
it performs about as well as the LSTM model. There is only a 0.2\% difference in predictive error,
and the t-test supports the claim that it does not impair the performance of the LSTM model.

By running experiments on a dataset with low noise and ok noise, it is clear that the dataset used with the hybrid model has a strong influence on performance.
Low noise is shown to reduce the performance of the hybrid model, while the problem does not occur in data with medium/ok noise.
Despite this, it is not clear if the amount of noise in the data only can impair the performance,
or if a more suitable amount of noise could possibly improve the performance.
In order to explore if this is the case we have also explored a dataset with a high level of noise as defined in \Cref*{section:Results:AdditionalExperimentalPlan}.
Unlike the datasets with low or ok noise, the high noise dataset proved to have more success with the CNN-AE-LSTM.
On average, the hybrid model performs a little under 4 percent better than the LSTM model using the sMAPE error metric.
Although the t-test is not able to verify the confidence interval that the predictions are significantly different between the LSTM and the hybrid model,
we are able to manually evaluate the predictions made by the two models on the dataset.
Although there are some variations in the predictions, we are able to see a general trend of the hybrid model increasing the performance over the LSTM model on the high variance.

Analyzing the results from the model predictions on the three datasets with high, low, and ok variance,
it is made clear that the dataset used in connection with the CNN-AE-LSTM and LSTM
has a strong influence on the predictive ability of the model.
Higher variance is easier for the model to predict and recreate meaningfully,
while data with lower noise serve as more of a problem with the autoencoder.

Considering the level of variance/noise in the dataset has such an impact on the hybrid model, it
is clear that other factors could also have a significant implication on the use of the hybrid model.
The dataset used by \cite{Zhao2019} could contain other characteristics that make the hybrid model well suited for predictions,
something that might not be available in the dataset from ``Prisguiden.no''.

One theory is that the more clearly seasonal data used in the paper by \cite{Zhao2019} strongly influence the results.
The traffic flow dataset illustrated in the paper as one of the datasets used had a clear and repetitive seasonal
pattern that occurs much more frequently than in our available dataset.
This could be an advantage for the autoencoder as it would have more similar data to work with and, therefore more easily
reconstruct data without data noise that is generally not contained in the rest of the dataset.

While these assumptions are difficult to test and prove in the given state of afears,
this is an important point for further investigation and work with the hybrid model in order
to test and verify its viability of use in other contexts or problems.






