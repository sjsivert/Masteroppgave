
\subsection{Modeling seasonality}
Our empirical results indicate that LSTMs have trouble modeling yearly seasonality.
The dataset with the least yearly seasonality is dataset 1. This is the only dataset where
the local univariate LSTM outperformed SARIMA. However, feeding the LSTM with additional
data such as day of the week, month, and season, it became significantly better on datasets
with a strong seasonal component.
Our findings do not contradict the findings of \cite{Hewamalage2021} regarding NNs ability
to model seasonality [\Cref{section:Data:Preprocessing:trend-and-seasonality}]
because all their datasets have a much higher seasonal frequency. Their
data show clear seasonal patterns in a plot with 50 time steps. In contrast, we have yearly seasonality
so we have to plot 365+ time steps to be able to see a seasonal pattern.

It seems LSTMs do not have long enough memory to model seasonal patterns
with a 365 timestep wavelength.
This hypothesis is supported by the findings of \cite{Zhao2020}, which concludes that
RNNs and LSTMs do not have long memory from a time series perspective.
But they do not give a definite answer for how long a LSTM remembers.

This explains why SARIMA outperforms the local univariate LSTM on dataset 2 and 3, as both includes
of time series with a strong seasonal dependence.
Experiment 5 [\Cref{section:results:additional-experimental-plan:Experiment-5}] where we removed
the seasonality from dataset 3 confirms this as well, but the experiment had too few samples
to prove anything statistically. When we remove trend and seasonality by differencing the training data,
we have to reincorporate this difference on our forecasted values before we can compare them with our
test targets. This is done iteratively by \Cref{eq:differencing-inverted} where the first
$z(t_1)$ is our first predicted value and our first observed $y(t_{n-1})$ value is the last value
we have before the forecast horizon. This process is based on only one real observation, which means
that we accumulate an error for each iteration, which can hurt the forecast accuracy.
This probably explains the poor results on dataset 1 when differencing is performed in Experiment 5.

Regarding RQ4 [\ref{RQ4}], we can conclude that on our dataset, when
both SARIMA and LSTM are given the exact same information, SARIMA will outperform
LSTM on time series with a strong 365-day seasonality because of the spatial memory limitations
of LSTMs.
SARIMA is a statistical method that does not rely on memory.
It requires the seasonal component for each time series to be known.
When a time series does not have a strong seasonal dependence or this
dependence is removed beforehand, the LSTM performs best.

Regarding RQ4.1 [\ref{RQ4}] our empirical results show that adding date stamp features to
a univariate series can significantly improve forecasting accuracy, especially
on time series with a strong seasonal dependence, and where this seasonal component
has a long wavelength.


%% Globale metoder gjør det bedre enn locale på MASE, men dårligere på sMAPE
% Kan det være fordi sMAPE straffer under predictions hardere enn 
% over predictions? TODO: Kjøre eksperiment på nytt for å få figures.
