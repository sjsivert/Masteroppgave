
\subsection{Convolutional Autoencoder}
\label{section:Method:CNN-AE-LSTM:AE}

% --------------------------------------------
% __ Contents __
% What model is used?
% What dataset is used? (Is there a split?)
% Error metrics used and recorded?
% Tuning method used
% Experiments run after tuning
% Expectations from experiments
% Add which resarch questions this answers or helps to answer
% --------------------------------------------


% What model is used
As part of the hybrid convolutional autoencoder and LSTM model structure,
a convolutional autoencoder is needed.
The autoencoder is intended to encode and reconstruct the input values of a time series,
before the reconstructed values can be used as input for the LSTM model.

The model is created using 1D convolutional and trans-convolutional layers,
encoding the spacial data from the time-series.


% Model selection
\subsubsection{Model selection}

In order to find a well suited autoencoder design manual tuning of the model was conducted.
Tuning of the model was done incrementally, with different compositions of layer types and sizes.
Using layers such as the convolutional 1 dimentional layer, dense layers, MinMaxPooling, BatchNormalization, and different dropouts,
an ideal model architecture was attempted.

After tuning the autoencoder on the different datasets with both local, global, univariate and mutlivariate experiments,
a shared model design was reached.
The convolutional autoencoder consists of an encoder component using 2 convolutional layers.
The first layer has a kernal size of 3, with 16 filters, while the secound layer has a kernel size of 5, with 32 filters.
Similarly, the decoder is comprised of two TransConvolutional layers with kernel sizes 5 and 3, where the first layer has a filter size of 32.
The number of filters in the last layer is dependent of the type of model created. A univariate model has only 1 filter, as only one value of reconstructed per time point,
while the multivariate model has 4.

This model architecture ensures the data is well reconstructed,
and can be shared across all the different models.



\subsubsection{Performance metric}
Due to the fact that the only aim for the autoencoder is to construct a recreation of the time series data,
it has no need for the same measure of accuracy such as the SARIMA model and the LSTM model.
This is because it does not performe future predictions, and it is therefor not compared to the other models.
However, it has its own goals which it aims to achieve.
In order to tune the autoencoder, a loss function is selected to be used duing the training process.
Taking inspiration from \cref{Zhao2019}, the error metrics MAPE was selected attempted.
However, during tuning and testing of the models, the MAPE metric was found to make predictions with exctreamly high error values.
The same goes for the MSE metric.
Therefore, based on experimentation and results from tuning,
the MAE metric was found to be the best match for the autoencoder.


\subsubsection{Local and global models}
With the current design of the autoencoder, the aim was to find an autoencoder which work well with both local an global models.
Even tough the global models is tasked with encoding more data, this was not a clear problem with the design, although there are limitations.
With highly correlating data, such as dataset 1, less data is needed to encoder because partial data is shared accross the different time series.
However, this is not the case for non-correlating data, such as with dataset 2.
With this, more data is needed to be encoded with the same model as above.
Althoug this impared the performance of the autoencoder to some degree, testing and tuning of models found that there are minimal difference in performance between the global and local models.
Therefor, both due to limitations with time, and with the results from model selection and tuning,
the same model was selected as a good fit for all cases, and therefor shared accross all the models.

\subsubsection{Univariate and multivariate}
Same as with the use of local and global models,
the use of univariate and multivariate models with the autoencoder served as a challange.
The selected model needed to be able to encode and reconstruct data for both univariate and multivariate data sources.

The same autoencoder model as described above is still used for both of these cases.
Although more data is in need of encoding and reconstruction,
the current autoencoder design works well.
Due to experimentation with univariate and multivariate models,
it is found that the model works well and can be used on all methods irregardles of univariate or multivariate.



% Same as with the autoencoder. More to learn for the univariate ae, but no global makes it dificult.

