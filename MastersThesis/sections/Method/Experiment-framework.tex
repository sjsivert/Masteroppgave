\section{Experiment Framework}
\label{section:method:experiment-framework}
In order to make it easier to execute multiple experiments, which all saved
enough metadata for the experiment to be recreatable and repeatable we made
a framework. It can be deconstructed into
four main modules. The configuration module, a data processing module, an experiment module, and the save experiment module.


\subsection{Pipeline module}
\label{section:Method:Pipeline}
The data processing module has two functions.
The first one is to streamline all data processing steps and structure them in one shared place.
Secondly, to have a self-documenting data module that can easily save
every processing step applied to the data before the experiments are executed.
Examples of a pipeline steps output are shown in \Cref{table:base_data_processing_steps}
and in \Cref{table:lstm_data_processing_steps}.

\begin{table}[h]
  \caption{Base data processing steps}
  \label{table:base_data_processing_steps}
  \begin{tabular}{ll}
    \toprule
    Step       & Description                                                   \\
    \midrule
    \textbf{1} & load market insight data and categories and merge them        \\
    \textbf{2} & convert date columns to date\_time format                     \\
    \textbf{3} & sum up clicks to category level [groupBy(date, cat\_id)]      \\
    \textbf{4} & rename column 'title' to 'cat\_name'                          \\
    \textbf{5} & combine feature 'hits' and 'clicks' to new feature 'interest' \\
    \textbf{6} & drop columns 'hits' and 'clicks'                              \\
    \textbf{7} & filter out data from early 2018-12-01                         \\
    \textbf{8} & drop uninteresting colums                                     \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{LSTM data processing steps}
  \label{table:lstm_data_processing_steps}
  \begin{tabular}{ll}
    \toprule
    Step       & Description                                                                  \\
    \textbf{1} & Convert input dataset to generator object                                    \\
    \textbf{2} & filter out category                                                          \\
    \textbf{3} & choose columns 'interest' and 'date'                                         \\
    \textbf{4} & fill in dates with zero values                                               \\
    \textbf{5} & convert to np.array                                                          \\
    \textbf{6} & scale data using standardization                                             \\
    \textbf{7} & generate x y pairs with sliding window with input size 10, and output size 7 \\
    \textbf{8} & generate training and validation data with training size 7                   \\
    \bottomrule
  \end{tabular}
\end{table}

% TODO: Do we need the ARIMA pipline steps as well? I think it might be overkill.
% commented out for now..
%\begin{table}[h]
%  \centering
%  \caption{LSTM data processing steps}
%  \label{table:arima_data_processing_steps}
%  \begin{tabular}{ll}
%    \toprule
%    Step       & Description                                                     \\
%    \textbf{1} & Convert input dataset to generator object                       \\
%    \textbf{2} & filter out category 2)                                          \\
%    \textbf{3} & choose columns 'interest' and 'date'                            \\
%    \textbf{4} & fill in dates with zero values                                  \\
%    \textbf{5} & Scaling data?: False                                            \\
%    \textbf{6} & split up into training set and test set of forecast window size \\
%    \bottomrule
%  \end{tabular}
%\end{table}

% TODO: What is the functionality of the pipeline, and why is it created?
% TODO: Add illustration of the project pipeline

%With the aim of running several experiments with ease, a experiment pipeline was created in this project.
%Running a project is done through the use of the shared pipeline, through the use of different data pipelines, model structure and model implementations, and different configureations.

Multiple pipelines were created in order to support all the different experiments.
While the deep learning methods such as the LSTM and the hybrid model only need two shared pipelines,
one for univariate and one for multivariate,
other experiments such as the SARIMA is in need of other pipelines.
These pipelines are designed with the aim of modularity,
making the expansion of multiple pipelines with multiple shared steps easy.




\subsection{Config module}
\label{section:method:experiment-framework:config-module}
The configuration module is created with the aim of making model selection and configuration more easily accessible.
Using separate configuration files for model configurations,
the amount of code that needs to be changed for the different experiments is reduced.
Additionally, by logging the configuration of models for each experiment,
backtracking the results of different configurations are more accessible.

The configuration is done through the use of two separate ``.yaml'' files.
The first one is the ``config.yaml'' file.
This contains information that is shared among all of the different experiments,
such as the selected data files, random seeds, logged error metrics etc.
Secondly, a separate configuration file contains the information more relevant to the specific experiment.
These configuration files contain information such as
the selected predictive model for the experiment,
as well as the model hyperparameters and tuning parameters.

During runtime, the two configuration files are merged, adding the information from the model configuration to the more general experiment configuration.
The config is parsed through at runtime, accessing the needed information when it becomes relevant.
A config example is available in the appendix at \Cref{cha:experiment-framework-example-config}.
Configs are also available in the source code.




\subsection{Save Experiment module}
\label{section:method:experiment-framework:save-experiment-module}

The save source module handled everything related to logging and saving an experiment.
Each experiment is associated with a unique descriptive ID and a description text to easily
differentiate between experiments.
Each experiment saves trained models, configs, the dataset used, stdout logs, training metrics, validation metrics,
testing metrics and figures.

Everything is save to the local ``./models/`` folder, and to an exernal ML experiment tracking tool
named \textit{neptune.ai}. The Neptune experiments are public and can be
seen at \textit{https://app.neptune.ai/sjsivertandsanderkk/Masteroppgave/}.
The local folder structure of a saved experiment is shown below.

\dirtree{%
  .1 arima-predict-cat-11037-7-days .
  .2 Arima-11037.pkl .
  .2 data-processing-steps.txt .
  .2 datasets.json .
  .2 figures .
  .3 11037-Data-Prediction.png .
  .3 11037-Predictions.png .
  .3 11037-Training-data-approximation.png .
  .3 11037-Training-data.png .
  .2 logging .
  .2 training-errors.csv .
  .2 metrics.txt .
  .2 options.yaml .
  .2 predictions.csv .
  .2 tags.txt .
  .2 title-description.txt .
}
2 directories, 13 files

\subsection{Packages and verions}

\begin{table}[h]
  \centering
  \caption{Experiment Python packages and versions}
  \label{table:python-packages-most-important}
  \begin{tabular}{|l|l|}\hline
    Package                      & Version \\ \hline
    \hline
    matplotlib                   & 3.5.1   \\
    matplotlib-inline            & 0.1.3   \\
    numpy                        & 1.22.2  \\
    pandas                       & 1.4.0   \\
    pandocfilters                & 1.5.0   \\
    sklearn                      & 0.0     \\
    statsmodels                  & 0.13.1  \\
    torch                        & 1.10.2  \\
    optuna                       & 2.10.0  \\
    plotly                       & 5.6.0   \\
    pytorch-lightning            & 1.5.10  \\
    keras                        & 2.8.0   \\
    tensorflow                   & 2.8.0   \\
    tensorflow-io-gcs-filesystem & 0.24.0  \\
    optkeras                     & 0.0.7   \\
    pmdarima                     & 1.8.5   \\
    \hline
  \end{tabular}
\end{table}
\Cref{table:python-packages-most-important} show the most important
python packages used for the experimental setup and their respective versions.
The full list of packages and versions are supplied in the
\textit{requirements.txt} file at the github code repository \cite{githubSource}
% \Cref{table:python-packages-all}