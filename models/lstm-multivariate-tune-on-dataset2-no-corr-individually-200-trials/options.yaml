data:
    data_path: ./datasets/raw/market_insights_overview_all_2022_02_14.csv
    categories_path: ./datasets/raw/solr_categories_all_2022_02_14.csv
logger:
    log_level: INFO
    log_file: ./log-file.log
use_gpu_if_available: no
experiment:
    tags: [market_insights]
    save_sources_to_use: [disk, neptune]
    checkpoint_save_location: ./models/0_current_model_checkpoints/
    log_model_every_n_epoch: 10
    error_metrics:
    - MAE
    - MASE
    - MSE
    - SMAPE
    - MAPE
    save_source:
        disk:
            model_save_location: ./models/
        neptune:
            project_id: sjsivertandsanderkk/Masteroppgave
model:
    model_type: local_univariate_lstm
    rng_seed: 42
    validation_model:
        placeholder: 0
    univariate_lstm:
        hyperparameter_tuning_range:
            hidden_size: [2, 100]
            number_of_layers: [1, 4]
            dropout: [0.0, 0.4]
            optimizer_name: [RMSprop, Adam]
            learning_rate: [1e-7, 1e-2]
            number_of_epochs: [5, 40]
            batch_size: [32, 32]
            input_window_size: 10
            output_window_size: 7
            multi_variable_nr: 4
            number_of_features: 4
            number_of_trials: 200
            stateful_lstm: yes
        global_model:
            parameters_for_all_models:
                training_size: 50
                input_window_size: 10
                output_window_size: 7
                multi_variable_nr: 4
                batch_size: 32
                number_of_epochs: 22
                stateful_lstm: yes
                should_shuffle_batches: no
                optimizer_name: Adam
                learning_rate: 0.00043177648728211254
                hidden_layer_size: 50
                dropout: 7.057973795771e-06
                number_of_features: 4
                number_of_layers: 1
            datasets:
            - 12532
            - 11694
            - 11716
            - 11950
            - 11195
            - 11998
            - 274
            - 11407
            - 46
            - 11326
            - 11335
            - 12197
            - 11693
            - 11780
            - 12502
            - 11866
            - 11400
            - 12256
            - 10320
            - 10030
        local_model:
            common_parameters_for_all_models:
                training_size: 50
                input_window_size: 10
                output_window_size: 7
                multi_variable_nr: 1
                batch_size: 32
                number_of_epochs: 15
                stateful_lstm: yes
                should_shuffle_batches: no
                optimizer_name: RMSprop
            model_structure:
            -   time_series_id: 12532
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 11694
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 11716
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 11950
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 11195
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 11998
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 274
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 11407
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 46
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 11326
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 11335
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 12197
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 11693
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 11780
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 12502
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 11866
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 11400
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 12256
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 10320
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
            -   time_series_id: 10030
                learning_rate: 0.00013756
                hidden_layer_size: 94
                dropout: 0.21199
                number_of_features: 1
                number_of_layers: 1
    local_univariate_cnn_ae:
        common_parameters_for_all_models:
            training_size: 7
            input_window_size: 14
            output_window_size: 7
            multi_variable_nr: 1
            batch_size: 1
            number_of_epochs: 51
            optimizer_name: adam
            loss: mse
            should_shuffle_batches: yes
        model_structure: [{time_series_id: 12322, learning_rate: 0.003, encoder: [{layer: Conv1d, filters: 8, kernel_size: 3, activation: relu}, {layer: MaxPool, size: 2, padding: valid}, {layer: Conv1d, filters: 16, kernel_size: 3}], decoder: [{layer: Conv1d, kernel_size: 5, filters: 8, activation: relu}, {layer: Conv1d, kernel_size: 3, filters: 4, activation: relu}, {layer: Conv1d, kernel_size: 3, filters: 1}]}]
    local_univariate_cnn_ae_lstm:
        common_parameters_for_all_models:
            should_shuffle_batches: yes
            training_size: 7
            optimizer_name: Adam
            loss: mae
            batch_size: 10
            epochs: 20
            number_of_epochs: 20
            input_window_size: 10
            output_window_size: 7
            multi_variable_nr: 1
            lstm-shared:
                input_window_size: 10
                output_window_size: 7
                multi_variable_nr: 1
                epochs: 26
        model_structure: [{time_series_id: 12322, lstm: {optimizer_name: RMSprop, stateful_lstm: yes, loss: mae, learning_rate: 7.88e-05, hidden_layer_size: 14, dropout: 0.132, number_of_features: 1, number_of_layers: 3}, ae: {optimizer_name: Adam, loss: mse, learning_rate: 0.003, epochs: 20}, encoder: [{layer: Conv1d, filters: 8, kernel_size: 3, activation: relu}, {layer: MaxPool, size: 2, padding: valid}, {layer: Conv1d, filters: 16, kernel_size: 3}], decoder: [{layer: Conv1d, kernel_size: 5, filters: 8, activation: relu}, {layer: Conv1d, kernel_size: 3, filters: 4, activation: relu}, {layer: Conv1d, kernel_size: 3, filters: 1}]}]
    local_univariate_arima:
        forecast_window_size: 7
        steps_to_predict: 7
        multi_step_forecast: yes
        auto_arima: yes
        seasonal: yes
        hyperparameter_tuning_range:
            p: [1, 3]
            d: [1, 3]
            q: [1, 3]
            P: [0, 3]
            D: [0, 3]
            Q: [0, 3]
            s: [12, 12]
        metric_to_use_when_tuning: MAE
        model_structure: [{time_series_id: 11573, hyperparameters: {p: 7, d: 3, q: 1, P: 3, D: 2, Q: 1, s: 12}}]
