\subsection{Error metric}

% Hva er den eller de valgte error metrics som brukes i denne oppgaven?
% Hva er grunnen til at det er disse vi har valgt?

% Her blir det mest sansynelig mye linking til tidlgiere arbeid og diskusjon rundt dette.
% Det er tydelig vi må ha en error metric som også kan brukes av ARIMA for å se at vår modell har lavere error
% Vi burde også bruke en error metric som måler hvordan vi predikerer ekstreme verdier!
% Anomalies og Extreme values er spessielt viktig å måle her kan det tenkes
% Det er foretrukket at det er litt teori bak valget, men det virker som om det å bruke flere error metrics ikke er en dårlig ting.
% Kan være fordelaktig å bruke flere for å måle modellen vår opp mot andre modeller.

% Her regner jeg med at du (Sindre) har en del mer å komme med enn meg
% Hva er grunnen til at error metrics er valgt andre steder, og hvordan kan vi bruke dette selv?
Since we are working with multiple time series and each time series might differ in scale.
When choosing an error metric, we have to accommodate a number of factors.
Since our time series are of different scales, we can't use a scale-dependent metric
like MSE or MAE.

Our data consists of positive integers. The possibility of zero-values does exist,
so SMAPE with the division-by-zero-fix as described in \autoref{section:BT:Loss} is a good candidate.
But SMAPE lacks interpretability and has a high skewness.

Together with SMAPE we suggest using the seasonal MASE because it seems to be the
state-of-the-art error metric for multiple time series forecasting.

Because we use both MASE and SMAPE we can combine them to one score, like
they do in the M4 competition.

\begin{equation}
  \label{eq:OWA}
  OWA = \frac{1}{2} \frac{MASE}{MASE_{\text{naive}}} + \frac{1}{2} \frac{SMAPE}{SMAPE_{\text{naive}}}
\end{equation}
