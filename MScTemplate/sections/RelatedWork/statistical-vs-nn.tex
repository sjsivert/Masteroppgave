\section{Statistical methods VS Neural Nets}
If a time series is stationary, then using its statistical properties is shown to be an effective and computationally cheap method 
\cite{Makridakis2018}.
In a paper written by \citeauthor{Makridakis2018} they test statistical methods versus machine learning methods for 
forecasting time series.
They evaluate perfomance across multiple forecasting horizons using a large subset of 1045 montly time seires.
Among the the machine learning methods they evaluate are 
\begin{itemize}
  \item \textit{Bayesian Neural Network (BNN)}
  \item \textit{Multi-Layer Perceptron (MLP)}
  \item \textit{K-Nearest Neighbor regression (KNN)}
  \item \textit{Recurrent Neural Network (RNN)} and
  \item \textit{Long Short Term Memory neural network (LSTM)}.
\end{itemize}

Among the statistical methods evaluated are
\begin{itemize}
  \item \textit{Naive2}
  \item \textit{ETS}
  \item \textit{ARIMA}
\end{itemize}
They evaluate with different loss functions and metrics, with simple one-step-ahead and multiple steps ahead methods.

Their findings are that all the simple statistical methods outperformed all the ML methods in terms of accuracy.
The statistical methods also had a lot less model complexity and computational cost for training.

\todo[]{Define sMAPE?}
The best performing model was ETS with a sMAPE score of 7.12. The second-best model was ARIMA with a score
of 7.19. The third worst model was LSTM with a score of 11.67.

The paper \cite{Makridakis2018} highlights some of the drawbacks of ML methods. Especially, their complexity,
their lack of explainability and their inability to show certainty in their predictions.
However, the study has gotten some well-defined critic. The authors \citeauthor*{Cerqueira2019} points 
out that \cite{Makridakis2018} does their experiments on datasets of too-small sample size.
Their largest time series sample size among the 1045 datasets is 144, and their smallest is 118.
They hypothesize that these datasets are too small for machine learning methods to generalize properly.

Their contribution is doing a similar study on 90 univariate time series, in which 
all the datasets have a sample size above 1000. They evaluate statistical methods vs ML methods
at different sample sizes, to test weather the sample size matters.

The paper \cite{Cerqueira2019} concludes that sample size does matter a whole lot for ML methods.
Statistical methods outperformed ML methods up to around a sample size of 130. After that ML methods
in general outperformed the simpler methods.

% Skal refleksjon mot vår situasjon diskuteres her?
This is good news for us, as our datasets do have a sample size of well above 660 at this moment.
Depending on when we conduct the experiment, the number will grow to the range of 1095 to 1860,
as data is being gathered every day as we speak.

One drawback of the \cite{Makridakis2018} study is that it does not test any of our chosen ML methods.
They tested:
\begin{itemize}
  \item \textit{A rule-based model (RBR)}
  \item \textit{A random Forest method (RF)}
  \item \textit{Guassion Process regression (GP)}
  \item \textit{The Multivariate adative regression (MARS)}
  \item \textit{Generalized linear model (GLM)}
\end{itemize}
But the paper \cite{Hewamalage2021} did a thorough analysis of state-of-the-art RNNs, including LSTM.
They concluded that complex methods now have benefits over simpler statistical benchmarks in many forecasting situations.


\todo[]{Skrive om dette i motivation}
The paper \citet{Bandara2017} points out a that in non-stationary time series, the distant past is typically less
useful for forecast, as underlying patterns and relationships will have changed in the meantime.
% TODO: This hopefully not apply to us as our time seires do seem to have consistency


\subsection{Univariate vs Multivariate time series}
In the previes section we discussed how statistical methods can outperform ML methods on
time series without a large enough sample size.
However, \cite{Bandara2017} points out that statistical methods, like ARIMA, are bound to
univariate time series. In the world of Big Data and lots of time series that correlate with each other,
treating each time series separately, and forecasting each in isolation might miss the big picture.
The paper argues that the ability to make models that can be trained globally across all series
holds a competitive advantage over models like ARIMA and ETS.

The paper argues that when building global models for a time-series database, the models are
potentially trained across disparate series, which may be detrimental to the overall accuracy.
They suggest to build separate models for subgroups of time series.
These groups can be based on domain knowledge, which proved to be the best option. With absence of 
domain knowledge they propose an automatic grouping mechanism to cluster series togheter.

The paper tests their model on two competition datasets, and achieves competitive results.
On the CIF2016 dataset, the model outperforms all other models.
On the NN5 competition it ranks 6th overall, and achieves consistent improvements over the baseline LSTM model.
They conclude that exploiting similarities of multiple time series in one model
is a competitive method.

The paper \cite{Rabanser2020} has some good arguments when comparing univariate vs multivariate models.
Both multivariate and global univariate methods work on groups of time series, but global methods
has the advantage of being more applicable because it does not require observartions of multiple
time series at the time of forecasting.
Also multivariate time series models works on groups that are supposed to have some form of 
dependence between them, while global models work on any group.
But when there exist shuch a dependency the global method will not capture it directly.
\cite{Hewamalage2021} states in their \textit{7. Future directions} chapter that complex
forecasting scenarios, shuch as a retail sales forecast, the sales of different products
may be interdependent.
Forecasting in such a context may require a multivariate model.


\subsubsection{Global vs Local methods}
On the topic of having to foract many time series as a group this paper has a good overview 
\cite{Montero-Manso2021}.
The paper points to two big disadvantages for univariate time series on a cluster of series.
The number one shortcomming is sample size. The second is scalability.
Scalability is a problem when you have a group of time series as each series require a seperate model
that requires human intervention. Forecasting a cluster of time series in this manner is called
\textit{the local approach.}

A univariate alternative to a local approach is \textit{the global approach}
\citep{Rabanser2020}.
The global approach works by pooling the data of all series togheter and fit a single univariate forecasting function. It prevents over-fitting because of the larger sample size.
% source slainas, flunkert, gasthaus & Januschowski 2020
The global approach has been introduced to exploit the natural scenario where all series
in the set are similare or related. An example given by the authors are the demand of fictional
books follows a similar pattern for all subgenres, stores or cover designs.
The idea behind is the strong assumption that all the time series in the set
come from the same process.
And even when this assumption not true, it will pay off in foreasting accuracy.
%As recent studies show puzzlingly good performance of time series that cannot be considered related.
The paper \cite{Rabanser2020} concludes that global and local methods for forecasting
sets of time series are equally general. The global method is neither restrictive nor requires
similarity or relatedness in the set.
But they point out that generalization of global models assumes groups of independent time series.
Under heave dependence, global models lose their beneficial perfomance guarantees. 

The paper \cite{Hewamalage2021} comes to the same conclusion. Stating that 
even on datasets that involve many heterogeneous series, the strong modelling capabilities of RNNs can drive
them to perform competitevely in terms of forecasting accuracy.
\todo[]{Skrive om paperet hvor klustering er gjort på tidsserier}


%\subsubsection{Limitations of statistical methods}
%\todo[inline]{Skrive om limitations til statistiske metoder and
%* Univariate
%* stationary time series
%* Dealing with extreme values
%https://towardsdatascience.com/limitations-of-arima-dealing-with-outliers-30cc0c6ddf33
%}
%Pro ML: size matters \cite{Cerqueira2019}
%
%From \cite{Guen2019}:
%Traditional methods for time series forecasting include linear autoregressive models, such as
%ARIMA odel, and exponential smoothing, which both fall into the broad category of of linear 
%state space models (SSMs). These methods handle linear
%dynamics and stationary time series (or made stationary by temporal differences).
%However the stationarity assumption is not satisfied for many real world tmie series that can present
%abrupt changes of distribution...