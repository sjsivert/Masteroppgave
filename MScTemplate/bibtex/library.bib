Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{SLR-cutoff,
author = {{Sindre Sivertsen}, Sander Kilen},
howpublished = {https://northern-leech-f32.notion.site/0565316ebb3944fcb2aae22527b7c376?v=51a2b4567aca405b92967f2312eeb15c},
title = {{Structured Literature Review, Final paper cut-off}},
year = {2021}
}
@article{Rabanser2020,
abstract = {Time series modeling techniques based on deep learning have seen many advancements in recent years, especially in data-abundant settings and with the central aim of learning global models that can extract patterns across multiple time series. While the crucial importance of appropriate data pre-processing and scaling has often been noted in prior work, most studies focus on improving model architectures. In this paper we empirically investigate the effect of data input and output transformations on the predictive performance of several neural forecasting architectures. In particular, we investigate the effectiveness of several forms of data binning, i.e. converting real-valued time series into categorical ones, when combined with feed-forward, recurrent neural networks, and convolution-based sequence models. In many non-forecasting applications where these models have been very successful, the model inputs and outputs are categorical (e.g. words from a fixed vocabulary in natural language processing applications or quantized pixel color intensities in computer vision). For forecasting applications, where the time series are typically real-valued, various ad-hoc data transformations have been proposed, but have not been systematically compared. To remedy this, we evaluate the forecasting accuracy of instances of the aforementioned model classes when combined with different types of data scaling and binning. We find that binning almost always improves performance (compared to using normalized real-valued inputs), but that the particular type of binning chosen is of lesser importance.},
archivePrefix = {arXiv},
arxivId = {2005.10111},
author = {Rabanser, Stephan and Januschowski, Tim and Flunkert, Valentin and Salinas, David and Gasthaus, Jan},
eprint = {2005.10111},
file = {:home/archie/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rabanser et al. - 2020 - The Effectiveness of Discretization in Forecasting An Empirical Study on Neural Time Series Models(2).pdf:pdf},
issn = {2331-8422},
month = {may},
title = {{The Effectiveness of Discretization in Forecasting: An Empirical Study on Neural Time Series Models}},
url = {http://arxiv.org/abs/2005.10111},
year = {2020}
}
@techreport{AndersKofod-Petersen2018,
author = {{Anders Kofod-Petersen}},
pages = {7},
title = {{How to do a Structured Literature Review in computer science}},
url = {https://research.idi.ntnu.no/aimasters/files/SLR{\_}HowTo2018.pdf},
year = {2018}
}
@article{Ramos2015,
abstract = {Forecasting future sales is one of the most important issues that is beyond all strategic and planning decisions in effective operations of retail businesses. For profitable retail businesses, accurate demand forecasting is crucial in organizing and planning production, purchasing, transportation and labor force. Retail sales series belong to a special type of time series that typically contain trend and seasonal patterns, presenting challenges in developing effective forecasting models. This work compares the forecasting performance of state space models and ARIMA models. The forecasting performance is demonstrated through a case study of retail sales of five different categories of women footwear: Boots, Booties, Flats, Sandals and Shoes. On both methodologies the model with the minimum value of Akaike's Information Criteria for the in-sample period was selected from all admissible models for further evaluation in the out-of-sample. Both one-step and multiple-step forecasts were produced. The results show that when an automatic algorithm the overall out-of-sample forecasting performance of state space and ARIMA models evaluated via RMSE, MAE and MAPE is quite similar on both one-step and multi-step forecasts. We also conclude that state space and ARIMA produce coverage probabilities that are close to the nominal rates for both one-step and multi-step forecasts.},
author = {Ramos, Patr{\'{i}}cia and Santos, Nicolau and Rebelo, Rui},
doi = {10.1016/J.RCIM.2014.12.015},
file = {::},
issn = {0736-5845},
journal = {Robotics and Computer-Integrated Manufacturing},
keywords = {ARIMA models,Forecast accuracy,Keywords Aggregate retail sales,State space models},
month = {aug},
pages = {151--163},
publisher = {Pergamon},
title = {{Performance of state space and ARIMA models for consumer retail sales forecasting}},
volume = {34},
year = {2015}
}
@article{Weng2020,
abstract = {Purpose: The purpose of this paper is to design a model that can accurately forecast the supply chain sales. Design/methodology/approach: This paper proposed a new model based on lightGBM and LSTM to forecast the supply chain sales. In order to verify the accuracy and efficiency of this model, three representative supply chain sales data sets are selected for experiments. Findings: The experimental results show that the combined model can forecast supply chain sales with high accuracy, efficiency and interpretability. Practical implications: With the rapid development of big data and AI, using big data analysis and algorithm technology to accurately forecast the long-term sales of goods will provide the database for the supply chain and key technical support for enterprises to establish supply chain solutions. This paper provides an effective method for supply chain sales forecasting, which can help enterprises to scientifically and reasonably forecast long-term commodity sales. Originality/value: The proposed model not only inherits the ability of LSTM model to automatically mine high-level temporal features, but also has the advantages of lightGBM model, such as high efficiency, strong interpretability, which is suitable for industrial production environment.},
author = {Weng, Tingyu and Liu, Wenyang and Xiao, Jun},
doi = {10.1108/IMDS-03-2019-0170},
issn = {02635577},
journal = {Industrial Management and Data Systems},
keywords = {Deep learning,Gradient boosting machine,Sale forecast,Supply chain},
month = {jan},
number = {2},
pages = {265--279},
publisher = {Emerald Group Holdings Ltd.},
title = {{Supply chain sales forecasting based on lightGBM and LSTM combination model}},
url = {https://www.researchgate.net/publication/335955033{\_}Supply{\_}chain{\_}sales{\_}forecasting{\_}based{\_}on{\_}lightGBM{\_}and{\_}LSTM{\_}combination{\_}model},
volume = {120},
year = {2020}
}
@misc{decisionmatrix,
abstract = {Also called: Pugh matrix, decision grid, selection matrix or grid, problem matrix, problem selection matrix, opportunity analysis, solution matrix, criteria rating form, criteria-based matrix. A decision matrix evaluates and prioritizes a list of options. The team first establishes a list of weighted criteria and then evaluates each option against those criteria. This is a variation of the L-shaped matrix.},
annote = {https://northern-leech-f32.notion.site/d29255c7adfb401ea84b2395780f704a?v=11d2972e878741208cea06fae2868d9f},
author = {{Sindre Sivertsen}, Sander Kilen},
howpublished = {https://northern-leech-f32.notion.site/d29255c7adfb401ea84b2395780f704a?v=11d2972e878741208cea06fae2868d9f},
keywords = {Notion},
title = {{Decision Matrix}},
url = {http://asq.org/learn-about-quality/decision-making-tools/overview/decision-matrix.html{\%}0Ahttps://asq.org/quality-press/display-item?item=H1224},
year = {2021}
}
@article{VanHoa2021,
abstract = {The problem of predicting foreign currency exchange rate is the problem in which many researchers in forecasting community have been interested. The exchange rate changes occur hourly, even seconds, thus producing correlated time series. To enable accurate forecasting on such correlated time series data, this work proposes a deep learning model which combines two approaches: autoencoder and Long Short-Term Memory (LSTM) network. The model employs an LSTM-based autoencoder in order to extract features from input dataset well. The model also uses LSTM-based network as a forecaster, which provides accurate and robust forecasting. Experimental results on four exchange rate datasets suggest that the proposed model is effective and outperforms five other comparative methods: shallow neural network (ANN), deep belief network (DBN), convolutional neural network (CNN), LSTM network and another form of combining autoencoder and LSTM network.},
author = {{Van Hoa}, Tran and Anh, Duong Tuan and Hieu, Duong Ngoc},
doi = {10.1145/3460179.3460184},
file = {::},
isbn = {9781450388948},
journal = {ACM International Conference Proceeding Series},
keywords = {Foreign exchange rate,autoencoder,deep neural network,forecasting,long short term memory network},
month = {feb},
pages = {22--28},
publisher = {Association for Computing Machinery},
title = {{Foreign Exchange Rate Forecasting using Autoencoder and LSTM Networks}},
url = {https://doi.org/10.1145/3460179.3460184},
year = {2021}
}
@article{Guen2019,
abstract = {This paper addresses the problem of time series forecasting for
non-stationary signals and multiple future steps prediction. To handle this
challenging task, we introduce DILATE (DIstortion Loss including shApe and
TimE), a new objective function for training deep neural networks. DILATE aims
at accurately predicting sudden changes, and explicitly incorporates two terms
supporting precise shape and temporal change detection. We introduce a
differentiable loss function suitable for training deep neural nets, and
provide a custom back-prop implementation for speeding up optimization. We also
introduce a variant of DILATE, which provides a smooth generalization of
temporally-constrained Dynamic Time Warping (DTW). Experiments carried out on
various non-stationary datasets reveal the very good behaviour of DILATE
compared to models trained with the standard Mean Squared Error (MSE) loss
function, and also to DTW and variants. DILATE is also agnostic to the choice
of the model, and we highlight its benefit for training fully connected
networks as well as specialized recurrent architectures, showing its capacity
to improve over state-of-the-art trajectory forecasting approaches.},
archivePrefix = {arXiv},
arxivId = {1909.09020},
author = {Guen, Vincent Le and Thome, Nicolas},
eprint = {1909.09020},
file = {::},
journal = {Advances in Neural Information Processing Systems},
month = {sep},
publisher = {Neural information processing systems foundation},
title = {{Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models}},
url = {https://arxiv.org/abs/1909.09020v4},
volume = {32},
year = {2019}
}
@article{Jiang2021a,
abstract = {Using time series data to predict future sales changes of products is of great significance to every retailing company in terms of management and planning of resources. In order to find an effective method to improve the accuracy of sales forecasting of retail goods which strongly influenced by season and holiday, this paper analyzes the feasibility of traditional time series model, hybrid models based on time series model and machine learning model, and machine learning model in predicting Walmart sales. The Prophet model which decomposes trend, season, and holiday and the machine learning model-lightGBM model-are used to train and test Walmart supermarket sales data from 2011-01-29 to 2016-06-19, and use data from 2016-06-19 to 2016-08-14 to make prediction and empirical analysis. The results suggest that the Root Mean Square Error (RMSE) of the Prophet model and the LightLGB model are 0.694 and 0.617, respectively, indicating that the machine learning model performs well in the sales forecast of retail stores. This provides a new idea for retailers to forecast sales by category and region.},
author = {Jiang, Haichen and Ruan, Jiatong and Sun, Jianmin},
doi = {10.1109/ICBDA51983.2021.9403224},
journal = {2021 IEEE 6th International Conference on Big Data Analytics, ICBDA 2021},
keywords = {LightGBM model,Machine learning,Prophet model,Sales forecast,Time series model},
month = {mar},
pages = {69--75},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Application of Machine Learning Model and Hybrid Model in Retail Sales Forecast}},
year = {2021}
}
@article{Russel2012,
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Russel, Stuart and Norvig, Peter},
doi = {10.1017/S0269888900007724},
eprint = {9809069v1},
isbn = {0136042597},
issn = {0269-8889},
journal = {The Knowledge Engineering Review},
pmid = {20949757},
primaryClass = {arXiv:gr-qc},
title = {{Artificial Intelligence: A Modern Approach}},
year = {2012}
}
@book{Box2016,
abstract = {Stochastic models and their forecasting. The autocorrelation function and spectrum. Linear stationary models. Linear nonstationary models. Forecasting. Stochastic model building. Model estimation. Model diagnostic checking. Seasonal models. Transfer function model building. Transfer function models. Identification, fitting, and checking of transfer function models. Design of discrete control schemes. Design of feedforward and feedback control schemes. Some further problems in control.},
author = {Ziegel, Eric R. and Box, G. and Jenkins, G. and Reinsel, G.},
booktitle = {Technometrics},
doi = {10.2307/1269640},
isbn = {978-1-118-67502-1},
issn = {00401706},
number = {2},
pages = {238},
publisher = {John Wiley {\&} Sons},
title = {{Time Series Analysis, Forecasting, and Control}},
volume = {37},
year = {1995}
}
@book{Utlaut2008,
abstract = {applicability for this approach.},
author = {Utlaut, Theresa L.},
booktitle = {Journal of Quality Technology},
doi = {10.1080/00224065.2008.11917751},
isbn = {978-0-471-65397-4},
issn = {0022-4065},
number = {4},
pages = {476--478},
publisher = {Wiley-Interscience},
title = {{Introduction to Time Series Analysis and Forecasting}},
volume = {40},
year = {2008}
}
@article{Makridakis2018,
abstract = {Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.},
author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
doi = {10.1371/JOURNAL.PONE.0194889},
file = {::},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {Algorithms,Artificial intelligence,Computing methods,Forecasting,Neural networks,Preprocessing,Statistical methods,Support vector machines},
month = {mar},
number = {3},
pages = {e0194889},
publisher = {Public Library of Science},
title = {{Statistical and Machine Learning forecasting methods: Concerns and ways forward}},
url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194889},
volume = {13},
year = {2018}
}
@misc{Hayes,
author = {Kenton, Will},
booktitle = {Investopedia},
title = {{Time Series Definition}},
url = {https://www.investopedia.com/terms/t/timeseries.asp},
urldate = {2021-10-27},
year = {2020}
}
@misc{slrdatabase,
author = {{Sindre Sivertsen}, Sander Kilen},
howpublished = {https://northern-leech-f32.notion.site/0565316ebb3944fcb2aae22527b7c376?v=5863b9e4b1ef444489836d218b1fcfc3},
title = {{Structured Literature Review Database}},
url = {https://northern-leech-f32.notion.site/0565316ebb3944fcb2aae22527b7c376?v=5863b9e4b1ef444489836d218b1fcfc3},
year = {2021}
}
@article{Jiang2021b,
abstract = {Using time series data to predict future sales changes of products is of great significance to every retailing company in terms of management and planning of resources. In order to find an effective method to improve the accuracy of sales forecasting of retail goods which strongly influenced by season and holiday, this paper analyzes the feasibility of traditional time series model, hybrid models based on time series model and machine learning model, and machine learning model in predicting Walmart sales. The Prophet model which decomposes trend, season, and holiday and the machine learning model-lightGBM model-are used to train and test Walmart supermarket sales data from 2011-01-29 to 2016-06-19, and use data from 2016-06-19 to 2016-08-14 to make prediction and empirical analysis. The results suggest that the Root Mean Square Error (RMSE) of the Prophet model and the LightLGB model are 0.694 and 0.617, respectively, indicating that the machine learning model performs well in the sales forecast of retail stores. This provides a new idea for retailers to forecast sales by category and region.},
author = {Jiang, Haichen and Ruan, Jiatong and Sun, Jianmin},
doi = {10.1109/ICBDA51983.2021.9403224},
file = {:home/archie/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Ruan, Sun - 2021 - Application of Machine Learning Model and Hybrid Model in Retail Sales Forecast.pdf:pdf},
isbn = {9780738131672},
journal = {2021 IEEE 6th International Conference on Big Data Analytics, ICBDA 2021},
keywords = {LightGBM model,Machine learning,Prophet model,Sales forecast,Time series model},
month = {mar},
pages = {69--75},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Application of Machine Learning Model and Hybrid Model in Retail Sales Forecast}},
year = {2021}
}
@article{Hewamalage2021,
abstract = {Recurrent Neural Networks (RNNs) have become competitive forecasting methods, as most notably shown in the winning method of the recent M4 competition. However, established statistical models such as exponential smoothing (ETS) and the autoregressive integrated moving average (ARIMA) gain their popularity not only from their high accuracy, but also because they are suitable for non-expert users in that they are robust, efficient, and automatic. In these areas, RNNs have still a long way to go. We present an extensive empirical study and an open-source software framework of existing RNN architectures for forecasting, and we develop guidelines and best practices for their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series in the dataset possess homogeneous seasonal patterns; otherwise, we recommend a deseasonalisation step. Comparisons against ETS and ARIMA demonstrate that (semi-) automatic RNN models are not silver bullets, but they are nevertheless competitive alternatives in many situations.},
archivePrefix = {arXiv},
arxivId = {1909.00590},
author = {Hewamalage, Hansika and Bergmeir, Christoph and Bandara, Kasun},
doi = {10.1016/j.ijforecast.2020.06.008},
eprint = {1909.00590},
file = {::},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Best practices,Big data,Forecasting,Framework},
month = {jan},
number = {1},
pages = {388--427},
publisher = {Elsevier},
title = {{Recurrent Neural Networks for Time Series Forecasting: Current status and future directions}},
volume = {37},
year = {2021}
}
@phdthesis{Ding2019,
abstract = {Time series prediction is an intensively studied topic in data mining. In spite of the considerable improvements, recent deep learning-based methods overlook the existence of extreme events, which result in weak performance when applying them to real time series. Extreme events are rare and random, but do play a critical role in many real applications, such as the forecasting of financial crisis and natural disasters. In this paper, we explore the central theme of improving the ability of deep learning on modeling extreme events for time series prediction. Through the lens of formal analysis, we first find that the weakness of deep learning methods roots in the conventional form of quadratic loss. To address this issue, we take inspirations from the Extreme Value Theory, developing a new form of loss called Extreme Value Loss (EVL) for detecting the future occurrence of extreme events. Furthermore, we propose to employ Memory Network in order to memorize extreme events in historical records. By incorporating EVL with an adapted memory network module, we achieve an end-to-end framework for time series prediction with extreme events. Through extensive experiments on synthetic data and two real datasets of stock and climate, we empirically validate the effectiveness of our framework. Besides, we also provide a proper choice for hyper-parameters in our proposed framework by conducting several additional experiments.},
author = {Ding, Daizong and Zhang, Mi and Pan, Xudong and Yang, Min and He, Xiangnan},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3292500.3330896},
isbn = {9781450362016},
keywords = {Attention Model,DNN,Extreme Event,Extreme Events,Loss function,Memory Network,Time Series},
mendeley-tags = {DNN,Extreme Events,Loss function,Time Series},
pages = {1114--1122},
title = {{Modeling extreme events in time series prediction}},
url = {https://raw.githubusercontent.com/NikZy/tdt99-2020/master/Topic 2/Modeling Extreme Events in Time Series Prediction.pdf},
year = {2019}
}
@book{RobJHyndman2014,
abstract = {1. Guru: I wrote the book, done it for decades, now I do the conference circuit. 2. Expert: It has been my full time job for more than a decade. 3. Skilled: I have been doing it for years. 4. Comfortable: I understand it and have done it. 5. Learner: I am still learning. 6. Beginner: I have heard of it and would like to learn more. 7. Unknown: What is forecasting? Is that what the weather people do?},
author = {{Rob J Hyndman}},
isbn = {9780987507105},
number = {September},
pages = {138},
title = {{Forecasting: Forecasting: Principles {\&} Practice}},
url = {robjhyndman.com/uwa{\%}5Cnhttp://robjhyndman.com/papers/forecasting-age-specific-breast-cancer-mortality-using-functional-data-models/},
year = {2014}
}
@article{Bandara2019a,
abstract = {Generating accurate and reliable sales forecasts is crucial in the E-commerce business. The current state-of-the-art techniques are typically univariate methods, which produce forecasts considering only the historical sales data of a single product. However, in a situation where large quantities of related time series are available, conditioning the forecast of an individual time series on past behaviour of similar, related time series can be beneficial. Since the product assortment hierarchy in an E-commerce platform contains large numbers of related products, in which the sales demand patterns can be correlated, our attempt is to incorporate this cross-series information in a unified model. We achieve this by globally training a Long Short-Term Memory network (LSTM) that exploits the non-linear demand relationships available in an E-commerce product assortment hierarchy. Aside from the forecasting framework, we also propose a systematic pre-processing framework to overcome the challenges in the E-commerce business. We also introduce several product grouping strategies to supplement the LSTM learning schemes, in situations where sales patterns in a product portfolio are disparate. We empirically evaluate the proposed forecasting framework on a real-world online marketplace dataset from Walmart.com. Our method achieves competitive results on category level and super-departmental level datasets, outperforming state-of-the-art techniques.},
archivePrefix = {arXiv},
arxivId = {1901.04028},
author = {Bandara, Kasun and Shi, Peibei and Bergmeir, Christoph and Hewamalage, Hansika and Tran, Quoc and Seaman, Brian},
doi = {10.1007/978-3-030-36718-3_39},
eprint = {1901.04028},
file = {::},
isbn = {9783030367176},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Demand forecasting,E-commerce,LSTM,Time series},
pages = {462--474},
publisher = {Springer},
title = {{Sales demand forecast in E-commerce using a long short-term memory neural network methodology}},
volume = {11955 LNCS},
year = {2019}
}
@article{Khan2020,
abstract = {Due to industrialization and the rising demand for energy, global energy consumption has been rapidly increasing. Recent studies show that the biggest portion of energy is consumed in residential buildings, i.e., in European Union countries up to 40{\%} of the total energy is consumed by households. Most residential buildings and industrial zones are equipped with smart sensors such as metering electric sensors, that are inadequately utilized for better energy management. In this paper, we develop a hybrid convolutional neural network (CNN) with an long short-term memory autoencoder (LSTM-AE) model for future energy prediction in residential and commercial buildings. The central focus of this research work is to utilize the smart meters' data for energy forecasting in order to enable appropriate energy management in buildings. We performed extensive research using several deep learning-based forecasting models and proposed an optimal hybrid CNN with the LSTM-AE model. To the best of our knowledge, we are the first to incorporate the aforementioned models under the umbrella of a unified framework with some utility preprocessing. Initially, the CNN model extracts features from the input data, which are then fed to the LSTM-encoder to generate encoded sequences. The encoded sequences are decoded by another following LSTM-decoder to advance it to the final dense layer for energy prediction. The experimental results using different evaluation metrics show that the proposed hybrid model works well. Also, it records the smallest value for mean square error (MSE), mean absolute error (MAE), root mean square error (RMSE) and mean absolute percentage error (MAPE) when compared to other state-of-the-art forecasting methods over the UCI residential building dataset. Furthermore, we conducted experiments on Korean commercial building data and the results indicate that our proposed hybrid model is a worthy contribution to energy forecasting.},
author = {Khan, Zulfiqar Ahmad and Hussain, Tanveer and Ullah, Amin and Rho, Seungmin and Lee, Miyoung and Baik, Sung Wook},
doi = {10.3390/S20051399},
file = {::},
issn = {14248220},
journal = {Sensors 2020, Vol. 20, Page 1399},
keywords = {LSTM,autoencoder,buildings energy management,deep learning,energy consumption prediction,load forecasting,smart sensors},
month = {mar},
number = {5},
pages = {1399},
pmid = {32143371},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Towards Efficient Electricity Forecasting in Residential and Commercial Buildings: A Novel Hybrid CNN with a LSTM-AE based Framework}},
url = {https://www.mdpi.com/1424-8220/20/5/1399/htm https://www.mdpi.com/1424-8220/20/5/1399},
volume = {20},
year = {2020}
}
@article{Zunic2020,
abstract = {This paper presents a framework capable of accurately forecasting future sales in the retail industry and classifying the product portfolio according to the expected level of forecasting reliability. The proposed framework, that would be of great use for any company operating in the retail industry, is based on Facebook's Prophet algorithm and backtesting strategy. Real-world sales forecasting benchmark data obtained experimentally in a production environment in one of the biggest retail companies in Bosnia and Herzegovina is used to evaluate the framework and demonstrate its capabilities in a real-world use case scenario.},
author = {Zunic, Emir and Korjenic, Kemal and Kerim, Hodzic and Donko, Dzenana},
doi = {10.5121/IJCSIT.2020.12203},
issn = {09754660},
journal = {International Journal of Computer Science and Information Technology},
month = {apr},
number = {2},
pages = {23--36},
publisher = {Academy and Industry Research Collaboration Center (AIRCC)},
title = {{Application of Facebook's Prophet Algorithm for Successful Sales Forecasting Based on Real-world Data}},
volume = {12},
year = {2020}
}
@misc{searchtermtable,
author = {{Sindre Sivertsen}, Sander Kilen},
howpublished = {https://northern-leech-f32.notion.site/Search-Term-Research-4b29caa541dc4a8abae16a7ea1af3849},
title = {{Search Term Table}},
year = {2021}
}
@article{Zhao2019,
abstract = {Many time series data are characterized by strong randomness and high noise.The traditional predictive model is difficult to extract the characteristics of the data, and the prediction effect is not very good. Convolutional neural networks and autoencoder have a good effect on extracting data features. Combining these two techniques, a predictive model of a combination of convolutional autoencoder(CAE) and Long Short Term Memory (LSTM) is proposed to predict time-series data with high noise. First, a one-dimensional convolution is used in the encoding and decoding network of the autoEncoder to extract data features and then use Long Short-Term Memory(LSTM) to predict. The experimental results show that the prediction error of convolutional autoEncoder-Long Short Term Memory (CAE and LSTM) model is significantly lower than other models.},
author = {Zhao, Xia and Han, Xiao and Su, Weijun and Yan, Zhen},
doi = {10.1109/CAC48633.2019.8996842},
isbn = {9781728140940},
journal = {Proceedings - 2019 Chinese Automation Congress, CAC 2019},
keywords = {Autoencoder,Convolutional Neural Networks,Long Short Term Memory,Noises,Prediction,Time series},
month = {nov},
pages = {5790--5793},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Time series prediction method based on Convolutional Autoencoder and LSTM}},
year = {2019}
}
@article{Bandara2019,
abstract = {Generating accurate and reliable sales forecasts is crucial in the E-commerce business. The current state-of-the-art techniques are typically univariate methods, which produce forecasts considering only the historical sales data of a single product. However, in a situation where large quantities of related time series are available, conditioning the forecast of an individual time series on past behaviour of similar, related time series can be beneficial. Since the product assortment hierarchy in an E-commerce platform contains large numbers of related products, in which the sales demand patterns can be correlated, our attempt is to incorporate this cross-series information in a unified model. We achieve this by globally training a Long Short-Term Memory network (LSTM) that exploits the non-linear demand relationships available in an E-commerce product assortment hierarchy. Aside from the forecasting framework, we also propose a systematic pre-processing framework to overcome the challenges in the E-commerce business. We also introduce several product grouping strategies to supplement the LSTM learning schemes, in situations where sales patterns in a product portfolio are disparate. We empirically evaluate the proposed forecasting framework on a real-world online marketplace dataset from Walmart.com. Our method achieves competitive results on category level and super-departmental level datasets, outperforming state-of-the-art techniques.},
author = {Bandara, Kasun and Shi, Peibei and Bergmeir, Christoph and Hewamalage, Hansika and Tran, Quoc and Seaman, Brian},
doi = {10.1007/978-3-030-36718-3_39},
file = {::},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Demand forecasting,E-commerce,LSTM,Time series},
pages = {462--474},
publisher = {Springer},
title = {{Sales demand forecast in E-commerce using a long short-term memory neural network methodology}},
url = {https://www.webofscience.com/wos/woscc/full-record/WOS:000612961500039},
volume = {11955 LNCS},
year = {2019}
}
@article{Zhang2020,
abstract = {The accurate estimation of future network traffic is a key enabler for early warning of network degradation and automated orchestration of network resources. The long short-term memory neural network (LSTM) is a popular architecture for network traffic forecasting, and has been successfully used in many applications. However, it has been observed that LSTMs suffer from limited memory capacity problems when the sequence is long. In this paper, we propose a gated dilated causal convolution based encoder-decoder (GDCC-ED) model for network traffic forecasting. The GDCC-ED learns a vector representation in the encoder from historical network traffic series, in which gated dilated causal convolutions are adopted to expand the long-range memory capacity. Moreover, different types of features in various perspectives, including temporal-independent and temporal-related features, are incorporated. In the decoder, the GDCC-ED exploits an RNN with LSTM units to map the vector representation back to a variable-length target sequence. Besides, a sequence data augmentation technique is designed to solve the problem of data scarcity. Experimental results demonstrate that our model achieves superior performance than state-of-the-art algorithms by 11.6{\%}.},
author = {Zhang, Xin and You, Jiali},
doi = {10.1109/ACCESS.2019.2963449},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {Network traffic forecasting,dilated causal convolution,encoder-decoder,gated activations},
pages = {6087--6097},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Gated Dilated Causal Convolution Based Encoder-Decoder for Network Traffic Forecasting}},
volume = {8},
year = {2020}
}
@article{BenTaieb2011,
abstract = {Multi-step ahead forecasting is still an open challenge in time series
forecasting. Several approaches that deal with this complex problem have been
proposed in the literature but an extensive comparison on a large number of
tasks is still missing. This paper aims to fill this gap by reviewing existing
strategies for multi-step ahead forecasting and comparing them in theoretical
and practical terms. To attain such an objective, we performed a large scale
comparison of these different strategies using a large experimental benchmark
(namely the 111 series from the NN5 forecasting competition). In addition, we
considered the effects of deseasonalization, input variable selection, and
forecast combination on these strategies and on multi-step ahead forecasting at
large. The following three findings appear to be consistently supported by the
experimental results: Multiple-Output strategies are the best performing
approaches, deseasonalization leads to uniformly improved forecast accuracy,
and input selection is more effective when performed in conjunction with
deseasonalization.},
archivePrefix = {arXiv},
arxivId = {1108.3259},
author = {{Ben Taieb}, Souhaib and Bontempi, Gianluca and Atiya, Amir F. and Sorjamaa, Antti},
doi = {10.1016/j.eswa.2012.01.039},
eprint = {1108.3259},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Friedman test,Lazy Learning,Long-term forecasting,Machine learning,Multi-step ahead forecasting,NN5 forecasting competition,Strategies of forecasting,Time series forecasting},
month = {aug},
number = {8},
pages = {7067--7083},
publisher = {Elsevier Ltd},
title = {{A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition}},
url = {https://arxiv.org/abs/1108.3259v1},
volume = {39},
year = {2011}
}
@article{Cerqueira2019,
abstract = {Time series forecasting is one of the most active research topics. Machine learning methods have been increasingly adopted to solve these predictive tasks. However, in a recent work, these were shown to systematically present a lower predictive performance relative to simple statistical methods. In this work, we counter these results. We show that these are only valid under an extremely low sample size. Using a learning curve method, our results suggest that machine learning methods improve their relative predictive performance as the sample size grows. The code to reproduce the experiments is available at https://github.com/vcerqueira/MLforForecasting.},
archivePrefix = {arXiv},
arxivId = {1909.13316},
author = {Cerqueira, Vitor and Torgo, Luis and Soares, Carlos},
eprint = {1909.13316},
issn = {2331-8422},
month = {sep},
title = {{Machine Learning vs Statistical Methods for Time Series Forecasting: Size Matters}},
url = {http://arxiv.org/abs/1909.13316},
year = {2019}
}
@article{Montero-Manso2021,
abstract = {Global methods that fit a single forecasting method to all time series in a set have recently shown surprising accuracy, even when forecasting large groups of heterogeneous time series. We provide the following contributions that help understand the potential and applicability of global methods and how they relate to traditional local methods that fit a separate forecasting method to each series: • Global and local methods can produce the same forecasts without any assumptions about similarity of the series in the set. • The complexity of local methods grows with the size of the set while it remains constant for global methods. This result supports the recent evidence and provides principles for the design of new algorithms. • In an extensive empirical study, we show that purposely na{\"{i}}ve algorithms derived from these principles show outstanding accuracy. In particular, global linear models provide competitive accuracy with far fewer parameters than the simplest of local methods.},
author = {Montero-Manso, Pablo and Hyndman, Rob J.},
doi = {10.1016/J.IJFORECAST.2021.03.004},
file = {::},
issn = {0169-2070},
journal = {International Journal of Forecasting},
keywords = {Cross-learning,Forecasting,Generalization,Global,Local,Pooled regression,Time series},
month = {oct},
number = {4},
pages = {1632--1653},
publisher = {Elsevier},
title = {{Principles and algorithms for forecasting groups of time series: Locality and globality}},
volume = {37},
year = {2021}
}
@article{Bandara2017,
abstract = {With the advent of Big Data, nowadays in many applications databases
containing large quantities of similar time series are available. Forecasting
time series in these domains with traditional univariate forecasting procedures
leaves great potentials for producing accurate forecasts untapped. Recurrent
neural networks (RNNs), and in particular Long Short-Term Memory (LSTM)
networks, have proven recently that they are able to outperform
state-of-the-art univariate time series forecasting methods in this context
when trained across all available time series. However, if the time series
database is heterogeneous, accuracy may degenerate, so that on the way towards
fully automatic forecasting methods in this space, a notion of similarity
between the time series needs to be built into the methods. To this end, we
present a prediction model that can be used with different types of RNN models
on subgroups of similar time series, which are identified by time series
clustering techniques. We assess our proposed methodology using LSTM networks,
a widely popular RNN variant. Our method achieves competitive results on
benchmarking datasets under competition evaluation procedures. In particular,
in terms of mean sMAPE accuracy, it consistently outperforms the baseline LSTM
model and outperforms all other methods on the CIF2016 forecasting competition
dataset.},
archivePrefix = {arXiv},
arxivId = {1710.03222},
author = {Bandara, Kasun and Bergmeir, Christoph and Smyl, Slawek},
eprint = {1710.03222},
file = {::},
journal = {Expert Systems with Applications},
keywords = {Big data forecasting,LSTM,Neural networks,RNN,Time series clustering},
month = {oct},
publisher = {Elsevier Ltd},
title = {{Forecasting Across Time Series Databases using Recurrent Neural Networks on Groups of Similar Series: A Clustering Approach}},
url = {https://arxiv.org/abs/1710.03222v2},
volume = {140},
year = {2017}
}
@book{Geron2017,
abstract = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. Using concrete examples, minimal theory, and two production-ready Python frameworks—scikit-learn and TensorFlow—author Aur{\'{e}}lien G{\'{e}}ron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks.},
author = {G{\'{e}}ron, Aur{\'{e}}lien},
edition = {1},
isbn = {9781491962299},
publisher = {O'Reilly Media},
title = {{Hands-On Machine Learning with Scikit-Learn and TensorFlow}},
year = {2017}
}
@article{Bowen2020,
abstract = {In order to prevent merchants from occupying a large amount of cash flow due to unreasonable replenishment and the waste of resources and loss of profits caused by goods in short supply, a reasonable forecast of sales volume is particularly important. This article used the ARIMA-BP nonlinear combination model to predict the sale of merchants' goods. Two separate prediction models, the BP neural network and the ARIMA, were used to predict the sales volume in the next 5 days, and then a mean square error model was established to weight the fitting and predictive results of the two single predictions. The weight obtained is brought into the BP neural network model for training, and one is obtained The dynamic weight model is used to predict the dynamic weight of two single prediction models in the next five days. Finally, the test data was substituted into the model, and the normalized error is 0.05363. The research shows that the model can adapt to the problem of forecasting the sales volume of e-commerce goods, more stable, and the error of the combined prediction is much smaller than a single prediction.},
author = {Bowen, Tan and Zhe, Zhang and Yulin, Zhang},
doi = {10.1109/ICAICA50127.2020.9181926},
isbn = {9781728170046},
journal = {Proceedings of 2020 IEEE International Conference on Artificial Intelligence and Computer Applications, ICAICA 2020},
keywords = {ARIMA-BP Nonlinear Combination Forecasting Model,Dynamic Weights,Mean Square Error Model},
month = {jun},
pages = {133--136},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Forecasting method of e-commerce cargo sales based on ARIMA-BP model*}},
year = {2020}
}
@article{Laptev,
abstract = {Accurate time-series forecasting during high variance segments (e.g., holidays), is critical for anomaly detection, optimal resource allocation, budget planning and other related tasks. At Uber accurate prediction for completed trips during special events can lead to a more efficient driver allocation resulting in a decreased wait time for the riders. State of the art methods for handling this task often rely on a combination of univariate fore-casting models (e.g., Holt-Winters) and machine learning methods (e.g., random forest). Such a system, however, is hard to tune, scale and add exogenous variables. Motivated by the recent resurgence of Long Short Term Memory networks we propose a novel end-to-end recurrent neural network architecture that outperforms the current state of the art event fore-casting methods on Uber data and generalizes well to a public M3 dataset used for time-series forecasting competitions.},
author = {Laptev, Nikolay and Yosinski, Jason and {Erran Li}, Li and Smyl, Slawek},
file = {::},
journal = {International Conference on Machine Learning (ICML)},
pages = {1--5},
title = {{Time-series Extreme Event Forecasting with Neural Networks at Uber}},
url = {http://roseyu.com/time-series-workshop/submissions/TSW2017{\_}paper{\_}3.pdf{\%}0Ahttps://www.semanticscholar.org/paper/Time-series-Extreme-Event-Forecasting-with-Neural-Laptev-Yosinski/a9b077367a8ca4cfa8425db31dd339673ddf1579},
year = {2017}
}
